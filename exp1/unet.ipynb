{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/homebrew/lib/python3.11/site-packages (2.0.1)\n",
      "Requirement already satisfied: torchvision in /opt/homebrew/lib/python3.11/site-packages (0.15.2)\n",
      "Requirement already satisfied: opencv-python in /opt/homebrew/lib/python3.11/site-packages (4.8.1.78)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (4.66.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.11/site-packages (1.3.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/homebrew/lib/python3.11/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (from torchvision) (1.25.2)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/homebrew/lib/python3.11/site-packages (from torchvision) (10.0.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->torchvision) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision opencv-python tqdm scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetContractingBlock(nn.Module):\n",
    "    def __init__(self, input_channels: int, output_channels: int, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # two back to back convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=output_channels,\n",
    "                  kernel_size=(3, 3))\n",
    "        self.conv2 = nn.Conv2d(in_channels=output_channels, out_channels=output_channels,\n",
    "                               kernel_size=(3, 3))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetExpandingBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channels: int, output_channels: int, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.upsample = nn.ConvTranspose2d(in_channels=input_channels, out_channels=output_channels,\n",
    "                                        kernel_size=(2, 2), stride=2)\n",
    "        self.conv1 = nn.Conv2d(in_channels=output_channels, out_channels=output_channels,\n",
    "                               kernel_size=(3, 3))\n",
    "        self.conv2 = nn.Conv2d(in_channels=output_channels, out_channels=output_channels,\n",
    "                               kernel_size=(3, 3))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.upsample(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        depth = 4\n",
    "\n",
    "        # contracting path\n",
    "        self.contracting_blocks = []\n",
    "        start_out_channels = 64\n",
    "        for i in range(depth):\n",
    "            if i != 0:\n",
    "                next_out_channels = start_out_channels * 2\n",
    "                self.contracting_blocks.append(\n",
    "                    UNetContractingBlock(input_channels=start_out_channels, output_channels=next_out_channels)\n",
    "                )\n",
    "                start_out_channels = next_out_channels\n",
    "            else:\n",
    "                self.contracting_blocks.append(\n",
    "                    UNetContractingBlock(input_channels=3, output_channels=start_out_channels)\n",
    "                )\n",
    "\n",
    "        # intermediate conv block, no maxpool\n",
    "        self.intermediate_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=(3, 3)),\n",
    "            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=(3, 3)),\n",
    "        )\n",
    "\n",
    "        # expanding path\n",
    "        self.expanding_blocks = []\n",
    "        start_in_channels = 1024\n",
    "        for _ in range(depth):\n",
    "            next_in_channels = start_in_channels // 2\n",
    "            self.expanding_blocks.append(\n",
    "                UNetExpandingBlock(input_channels=start_in_channels, output_channels=next_in_channels)\n",
    "            )\n",
    "            start_in_channels = next_in_channels\n",
    "\n",
    "        # last convolution\n",
    "        self.conv_last = nn.Conv2d(in_channels=64, out_channels=2,\n",
    "                                   kernel_size=(1, 1,))\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        for block in self.contracting_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.intermediate_conv(x)\n",
    "\n",
    "        for block in self.expanding_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.conv_last(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UNetContractingBlock(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "), UNetContractingBlock(\n",
      "  (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "), UNetContractingBlock(\n",
      "  (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "), UNetContractingBlock(\n",
      "  (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")]\n",
      "[UNetExpandingBlock(\n",
      "  (upsample): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "), UNetExpandingBlock(\n",
      "  (upsample): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "), UNetExpandingBlock(\n",
      "  (upsample): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "), UNetExpandingBlock(\n",
      "  (upsample): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      ")]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Tensor,\n",
       " torch.Size([1, 2, 388, 388]),\n",
       " tensor([[[[ 0.0646,  0.0650,  0.0647,  ...,  0.0650,  0.0647,  0.0650],\n",
       "           [ 0.0644,  0.0656,  0.0644,  ...,  0.0656,  0.0644,  0.0656],\n",
       "           [ 0.0646,  0.0650,  0.0646,  ...,  0.0650,  0.0646,  0.0649],\n",
       "           ...,\n",
       "           [ 0.0644,  0.0656,  0.0644,  ...,  0.0656,  0.0644,  0.0656],\n",
       "           [ 0.0646,  0.0650,  0.0646,  ...,  0.0650,  0.0646,  0.0649],\n",
       "           [ 0.0644,  0.0655,  0.0644,  ...,  0.0655,  0.0644,  0.0656]],\n",
       " \n",
       "          [[-0.1277, -0.1275, -0.1278,  ..., -0.1275, -0.1278, -0.1275],\n",
       "           [-0.1276, -0.1277, -0.1277,  ..., -0.1277, -0.1277, -0.1277],\n",
       "           [-0.1278, -0.1276, -0.1277,  ..., -0.1276, -0.1277, -0.1275],\n",
       "           ...,\n",
       "           [-0.1276, -0.1277, -0.1277,  ..., -0.1277, -0.1277, -0.1277],\n",
       "           [-0.1278, -0.1276, -0.1277,  ..., -0.1276, -0.1277, -0.1275],\n",
       "           [-0.1276, -0.1277, -0.1277,  ..., -0.1277, -0.1277, -0.1277]]]],\n",
       "        grad_fn=<ConvolutionBackward0>))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.rand(1, 3, 572, 572)  # batch size, channels, height, width\n",
    "out = model(t)\n",
    "\n",
    "type(out), out.shape, out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_BASE = \"../../dataset/cellpose/\"\n",
    "train_path = os.path.join(DATASET_BASE, \"train\")\n",
    "test_path = os.path.join(DATASET_BASE, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellPoseDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, image_path, transforms) -> None:\n",
    "        self.image_path = image_path\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.images = []\n",
    "        self.masks = []\n",
    "        self.count = 0\n",
    "        for img in sorted(os.listdir(self.image_path)):\n",
    "            self.count += 1\n",
    "            if img.endswith(\"img.png\"):\n",
    "                self.images.append(img)\n",
    "            else:\n",
    "                self.masks.append(img)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.count\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # return tuple of image and mask: (image, mask)\n",
    "        image = cv2.imread(os.path.join(train_path, self.images[index]))\n",
    "        mask = cv2.imread(os.path.join(train_path, self.masks[index]))\n",
    "\n",
    "        transformed_image = self.transforms(image)\n",
    "        transformed_mask = self.transforms(mask)\n",
    "\n",
    "        return transformed_image, transformed_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 388\n",
    "IMG_WIDTH = 388"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CellPoseDataset(train_path, transform)\n",
    "\n",
    "test_ratio = 0.15\n",
    "dataset_size = len(dataset)\n",
    "test_size = int(dataset_size*0.15)\n",
    "\n",
    "train_size = dataset_size - test_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size,)\n",
    "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 388, 388]),\n",
       " (tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "  \n",
       "          [[0.0000, 0.0000, 0.0000,  ..., 0.0863, 0.1608, 0.1725],\n",
       "           [0.0118, 0.0118, 0.0118,  ..., 0.0510, 0.1373, 0.1725],\n",
       "           [0.0118, 0.0118, 0.0118,  ..., 0.0353, 0.1098, 0.1686],\n",
       "           ...,\n",
       "           [0.0157, 0.0157, 0.0157,  ..., 0.0196, 0.0235, 0.0275],\n",
       "           [0.0157, 0.0157, 0.0157,  ..., 0.0196, 0.0235, 0.0275],\n",
       "           [0.0157, 0.0157, 0.0157,  ..., 0.0196, 0.0235, 0.0235]],\n",
       "  \n",
       "          [[0.0078, 0.0039, 0.0039,  ..., 0.0078, 0.0157, 0.0118],\n",
       "           [0.0235, 0.0275, 0.0235,  ..., 0.0196, 0.0235, 0.0275],\n",
       "           [0.0275, 0.0314, 0.0275,  ..., 0.0235, 0.0275, 0.0275],\n",
       "           ...,\n",
       "           [0.0353, 0.0353, 0.0275,  ..., 0.0235, 0.0275, 0.0275],\n",
       "           [0.0353, 0.0314, 0.0314,  ..., 0.0235, 0.0275, 0.0275],\n",
       "           [0.0353, 0.0314, 0.0314,  ..., 0.0275, 0.0235, 0.0235]]]),\n",
       "  tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]])))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0].shape, dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1080"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (intermediate_conv): Sequential(\n",
       "    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
       "  )\n",
       "  (conv_last): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UNetContractingBlock(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "), UNetContractingBlock(\n",
      "  (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "), UNetContractingBlock(\n",
      "  (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "), UNetContractingBlock(\n",
      "  (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")]\n",
      "[UNetExpandingBlock(\n",
      "  (upsample): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "), UNetExpandingBlock(\n",
      "  (upsample): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "), UNetExpandingBlock(\n",
      "  (upsample): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "), UNetExpandingBlock(\n",
      "  (upsample): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      ")]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0646,  0.0650,  0.0647,  ...,  0.0650,  0.0647,  0.0650],\n",
       "         [ 0.0644,  0.0656,  0.0644,  ...,  0.0656,  0.0644,  0.0656],\n",
       "         [ 0.0646,  0.0650,  0.0646,  ...,  0.0650,  0.0646,  0.0649],\n",
       "         ...,\n",
       "         [ 0.0644,  0.0656,  0.0644,  ...,  0.0656,  0.0644,  0.0656],\n",
       "         [ 0.0646,  0.0650,  0.0646,  ...,  0.0650,  0.0646,  0.0649],\n",
       "         [ 0.0644,  0.0655,  0.0644,  ...,  0.0655,  0.0644,  0.0656]],\n",
       "\n",
       "        [[-0.1277, -0.1275, -0.1278,  ..., -0.1275, -0.1278, -0.1275],\n",
       "         [-0.1276, -0.1277, -0.1277,  ..., -0.1277, -0.1277, -0.1277],\n",
       "         [-0.1278, -0.1276, -0.1277,  ..., -0.1276, -0.1277, -0.1275],\n",
       "         ...,\n",
       "         [-0.1276, -0.1277, -0.1277,  ..., -0.1277, -0.1277, -0.1277],\n",
       "         [-0.1278, -0.1276, -0.1277,  ..., -0.1276, -0.1277, -0.1275],\n",
       "         [-0.1276, -0.1277, -0.1277,  ..., -0.1277, -0.1277, -0.1277]]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.03\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = len(train_dataset) // batch_size\n",
    "test_steps = len(test_dataset) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UNetContractingBlock(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "), UNetContractingBlock(\n",
      "  (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "), UNetContractingBlock(\n",
      "  (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "), UNetContractingBlock(\n",
      "  (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")]\n",
      "[UNetExpandingBlock(\n",
      "  (upsample): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "), UNetExpandingBlock(\n",
      "  (upsample): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "), UNetExpandingBlock(\n",
      "  (upsample): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "), UNetExpandingBlock(\n",
      "  (upsample): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      ")]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([3, 388, 388])) must be the same as input size (torch.Size([2, 196, 196]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/ogola/superurop/superurop-experiments/exp1/unet.ipynb Cell 26\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ogola/superurop/superurop-experiments/exp1/unet.ipynb#X33sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mfor\u001b[39;00m (x, y) \u001b[39min\u001b[39;00m test_dataset:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ogola/superurop/superurop-experiments/exp1/unet.ipynb#X33sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         pred \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ogola/superurop/superurop-experiments/exp1/unet.ipynb#X33sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         total_test_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_func(pred, y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ogola/superurop/superurop-experiments/exp1/unet.ipynb#X33sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m avg_train_loss \u001b[39m=\u001b[39m total_train_loss \u001b[39m/\u001b[39m train_steps\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ogola/superurop/superurop-experiments/exp1/unet.ipynb#X33sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m avg_test_loss \u001b[39m=\u001b[39m total_test_loss \u001b[39m/\u001b[39m test_steps\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/loss.py:720\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 720\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39;49m, target,\n\u001b[1;32m    721\u001b[0m                                               \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    722\u001b[0m                                               pos_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos_weight,\n\u001b[1;32m    723\u001b[0m                                               reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/functional.py:3163\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3160\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (target\u001b[39m.\u001b[39msize() \u001b[39m==\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()):\n\u001b[0;32m-> 3163\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTarget size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) must be the same as input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()))\n\u001b[1;32m   3165\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([3, 388, 388])) must be the same as input size (torch.Size([2, 196, 196]))"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for e in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "\n",
    "    total_train_loss = 0\n",
    "    total_test_loss = 0\n",
    "\n",
    "    # for i, (x, y) in enumerate(train_loader):\n",
    "    for i, (x, y) in enumerate(train_dataset):\n",
    "        pred = model(x)\n",
    "        loss = loss_func(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        # for (x, y) in test_loader:\n",
    "        for (x, y) in test_dataset:\n",
    "            pred = model(x)\n",
    "            total_test_loss += loss_func(pred, y)\n",
    "\n",
    "    avg_train_loss = total_train_loss / train_steps\n",
    "    avg_test_loss = total_test_loss / test_steps\n",
    "\n",
    "    print(f\"[INFO] EPOCH: {e + 1}/{epochs}\")\n",
    "    print(\"Train loss: {:.6f}, Test loss: {:.4f}\".format(avg_train_loss, avg_test_loss))\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"[INFO] total time taken to train the model: {end_time-start_time:.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
