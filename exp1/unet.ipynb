{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.10/site-packages (2.0.1)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.10/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetContractingBlock(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # two back to back convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=output_channels,\n",
    "                  kernel_size=(3, 3))\n",
    "        self.conv2 = nn.Conv2d(in_channels=output_channels, out_channels=output_channels,\n",
    "                               kernel_size=(3, 3))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetExpandingBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channels, output_channels, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.upsample = nn.ConvTranspose2d(in_channels=input_channels, out_channels=output_channels,\n",
    "                                        kernel_size=(2, 2), stride=2)\n",
    "        self.conv1 = nn.Conv2d(in_channels=output_channels, out_channels=output_channels,\n",
    "                               kernel_size=(3, 3))\n",
    "        self.conv2 = nn.Conv2d(in_channels=output_channels, out_channels=output_channels,\n",
    "                               kernel_size=(3, 3))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.upsample(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        depth = 4\n",
    "\n",
    "        # contracting path\n",
    "        self.contracting_blocks = []\n",
    "        start_out_channels = 64\n",
    "        for i in range(depth):\n",
    "            if i != 0:\n",
    "                next_out_channels = start_out_channels * 2\n",
    "                self.contracting_blocks.append(\n",
    "                    UNetContractingBlock(input_channels=start_out_channels, output_channels=next_out_channels)\n",
    "                )\n",
    "                start_out_channels = next_out_channels\n",
    "            else:\n",
    "                self.contracting_blocks.append(\n",
    "                    UNetContractingBlock(input_channels=1, output_channels=start_out_channels)\n",
    "                )\n",
    "\n",
    "        # intermediate conv block, no maxpool\n",
    "        self.intermediate_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=(3, 3)),\n",
    "            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=(3, 3)),\n",
    "        )\n",
    "\n",
    "        # expanding path\n",
    "        self.expanding_blocks = []\n",
    "        start_in_channels = 1024\n",
    "        for _ in range(depth):\n",
    "            next_in_channels = start_in_channels // 2\n",
    "            self.expanding_blocks.append(\n",
    "                UNetExpandingBlock(input_channels=start_in_channels, output_channels=next_in_channels)\n",
    "            )\n",
    "            start_in_channels = next_in_channels\n",
    "\n",
    "        # last convolution\n",
    "        self.conv_last = nn.Conv2d(in_channels=64, out_channels=2,\n",
    "                                   kernel_size=(1, 1,))\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        print(self.contracting_blocks)\n",
    "        print(self.expanding_blocks)\n",
    "        for block in self.contracting_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.intermediate_conv(x)\n",
    "\n",
    "        for block in self.expanding_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.conv_last(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UNetContractingBlock(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "), UNetContractingBlock(\n",
      "  (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "), UNetContractingBlock(\n",
      "  (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "), UNetContractingBlock(\n",
      "  (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")]\n",
      "[UNetExpandingBlock(\n",
      "  (upsample): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "), UNetExpandingBlock(\n",
      "  (upsample): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "), UNetExpandingBlock(\n",
      "  (upsample): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "), UNetExpandingBlock(\n",
      "  (upsample): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      ")]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Tensor,\n",
       " torch.Size([1, 2, 388, 388]),\n",
       " tensor([[[[-0.0152, -0.0153, -0.0153,  ..., -0.0153, -0.0153, -0.0153],\n",
       "           [-0.0155, -0.0151, -0.0155,  ..., -0.0151, -0.0155, -0.0152],\n",
       "           [-0.0152, -0.0153, -0.0152,  ..., -0.0153, -0.0152, -0.0153],\n",
       "           ...,\n",
       "           [-0.0155, -0.0151, -0.0155,  ..., -0.0151, -0.0155, -0.0152],\n",
       "           [-0.0152, -0.0153, -0.0152,  ..., -0.0153, -0.0152, -0.0153],\n",
       "           [-0.0155, -0.0151, -0.0155,  ..., -0.0151, -0.0155, -0.0151]],\n",
       " \n",
       "          [[-0.1414, -0.1413, -0.1414,  ..., -0.1413, -0.1414, -0.1413],\n",
       "           [-0.1414, -0.1415, -0.1414,  ..., -0.1415, -0.1414, -0.1415],\n",
       "           [-0.1414, -0.1414, -0.1414,  ..., -0.1414, -0.1414, -0.1413],\n",
       "           ...,\n",
       "           [-0.1414, -0.1415, -0.1414,  ..., -0.1415, -0.1414, -0.1415],\n",
       "           [-0.1414, -0.1414, -0.1414,  ..., -0.1414, -0.1414, -0.1413],\n",
       "           [-0.1414, -0.1416, -0.1415,  ..., -0.1416, -0.1415, -0.1416]]]],\n",
       "        grad_fn=<ConvolutionBackward0>))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.rand(1, 1, 572, 572)  # batch size, channels, height, width\n",
    "out = model(t)\n",
    "\n",
    "type(out), out.shape, out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
